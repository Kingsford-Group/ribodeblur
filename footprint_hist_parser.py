#!/usr/bin/env python
"""
parse read length histogram 
read counts grouped by length and transcript loci
"""
import sys
import collections
import Bio.SeqIO

def get_cds_range(fa_fname):
    """ get cds range from fasta headers """
    cds_range = {}
    for record in Bio.SeqIO.parse(fa_fname, "fasta"):
        tid = record.id
        if "CDS:" not in record.description:
            print("ERROR: {} not generated by build_reference.py!".format(fa_fname), file=sys.stderr)
            print("reference file does not contain CDS range information!", file=sys.stderr)
            sys.exit(1)
        words = record.description.split("|")
        for w in words:
            if w.startswith("CDS:"):
                begin, end = map(int, w.lstrip("CDS:").split("-"))
                cds_range[tid] = (begin, end)
                break
    return cds_range
    
def parse_rlen_hist(fname):
    print("parse footprint counts grouped by read length")
    tlist = {}
    transcript = {}
    tf = open(fname)
    i = 0
    line = tf.readline()
    while line:
        if line.startswith("refID: "):
            if transcript:
                rid = transcript["id"]
                i += 1
                sys.stdout.write("processed transcript {0}.\t\r".format(i))
                sys.stdout.flush()
                tlist[rid] = transcript.copy()
                transcript.clear()
            rid = int(line.lstrip("refID: ").rstrip("\n"))
            transcript["id"] = rid
        elif line.startswith("tid: "):
            tid = line.lstrip("tid: ").rstrip("\n")
            transcript["tid"] = tid
            transcript["prof"] = {}
        # profile len grouped by read length
        # format: read len: pos count, ...
        # [pos count] is the adjacency list way to save space
        else:
            read_len, pc_str = line.rstrip("\n").split(": ")
            read_len = int(read_len)
            prof = []
            pc_pairs = pc_str.rstrip(", ").split(", ")
            for pc in pc_pairs:
                pos, cnt = pc.split(" ")
                prof.append([int(pos), float(cnt)])
            transcript["prof"][read_len] = prof
        line = tf.readline()
    if transcript:
        rid = transcript["id"]
        tlist[rid] = transcript
    tf.close()
    sys.stdout.write("\n")
    return tlist

def get_transcript_profiles(tlist, cds_range, ibegin, iend):
    """ 
    create read length specific count profiles for the read start loci for each transcript
    ibegin: index bound to include before START (negative)
    iend: bases to include after STOP (positive)
    """
    print("create transcript profiles")
    tprofile = collections.OrderedDict()
    for idx, rid in enumerate(tlist.keys()):
        tid = tlist[rid]['tid']
        start, end = cds_range[tid]
        tlen = end-start
        for rlen in tlist[rid]['prof']:
            for pos, cnt in tlist[rid]['prof'][rlen]:
                i = pos - start
                if i < ibegin or i > tlen+iend : continue
                tprofile.setdefault(tid,{})
                tprofile[tid].setdefault(rlen, [])
                tprofile[tid][rlen].append((i,cnt))
        sys.stdout.write("processed transcript {0}.\t\r".format(idx))
        sys.stdout.flush()
    sys.stdout.write("\n")
    return tprofile

def write_rlen_hist(tprofile, cds_range, tid2rid, ofname):
    print("write filtered profile", file=sys.stderr)
    tf = open(ofname, 'w')
    i = 0
    for tid, plist in tprofile.items():
        start, stop = cds_range[tid]
        text = [ "refID: {0}\n".format(tid2rid[tid]), 
                 "tid: {0}\n".format(tid) ] + \
            [ "{0}: {1}\n".format(rlen, ", ".join(map(lambda x: "{0} {1:.0f}".format(x[0]+start, x[1]), pos_list))) for rlen, pos_list in plist.items() ]
        tf.writelines(text)
        i += 1
        sys.stderr.write("processed transcript {0}.\t\t\r".format(i))
        sys.stderr.flush()
    sys.stderr.write("\n")
    tf.close()
